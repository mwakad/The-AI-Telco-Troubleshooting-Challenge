{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The AI Telco Troubleshooting Challenge\n",
    "\n",
    "Reducing the operational cost of network faults - whether caused by hardware failures or software misconfigurations - is a critical priority for modern telecom service providers (telcos).\n",
    "\n",
    "Telelogs, the automatically generated fault and event logs produced by network equipment, offer a rich source of information. Recent research has demonstrated that these logs can be used to fine-tune specialised LLMs capable of performing root-cause analysis and assisting network engineers. However, building models that generalise across unseen faults, new data distributions, and entirely new network environments, while still running efficiently on constrained edge servers, is still a significant challenge.\n",
    "\n",
    "\n",
    "The evaluation metric for this challenge is Pass @ 1\n",
    "\n",
    "This metric measures the ability of the model to produce a correct answer in a single attempt. It is computed by evaluating each of the 4 generated responses individually and averaging the correctness over all samples.\n",
    "\n",
    "The models will be evaluated on their capability to troubleshoot network problems together with knowledge retention. Knowledge retention is the ability to maintain general knowledge accuracy after fine-tuning. The private dataset will include network faults whose data has a different structure than telelogs, and general knowledge questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.57.6)\n",
      "Requirement already satisfied: peft in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.18.1)\n",
      "Requirement already satisfied: bitsandbytes in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.49.1)\n",
      "Requirement already satisfied: trl in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.27.0)\n",
      "Requirement already satisfied: accelerate in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: datasets in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.5.0)\n",
      "Requirement already satisfied: filelock in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: requests in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: psutil in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from peft) (7.2.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from peft) (2.10.0)\n",
      "Requirement already satisfied: setuptools in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.6.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from cuda-bindings==12.9.4->torch>=1.13.0->peft) (1.3.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (23.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers) (2.6.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install neccesary libraries\n",
    "!pip install pandas numpy matplotlib seaborn\n",
    "!pip install transformers peft bitsandbytes trl accelerate datasets\n",
    "!pip install -qU flash-attn --no-build-isolation\n",
    "!pip install -qU vllm>=0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and data loading\n",
    "import re\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from io import StringIO\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "phase1_truth = pd.read_csv('phase_1_test_truth.csv')\n",
    "phase1_test = pd.read_csv(\"phase_1_test.csv\")\n",
    "phase2_test = pd.read_csv(\"phase_2_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train dataset\n",
    "\n",
    "# Remove the underscore followed by a digit at the end of each ID\n",
    "phase1_truth['ID'] = phase1_truth['ID'].str.replace(r'_\\d$', '', regex=True)\n",
    "phase1_truth = phase1_truth[['ID', 'Qwen2.5-1.5B-Instruct']]\n",
    "phase1_truth = phase1_truth.rename(columns={'Qwen2.5-1.5B-Instruct': 'answer'})\n",
    "phase1_truth = phase1_truth.drop_duplicates(subset=['ID'])\n",
    "questions_df = phase1_test[['ID', 'question']].copy()\n",
    "phase1_truth = phase1_truth.merge(questions_df, on='ID', how='left')\n",
    "phase1_truth = phase1_truth[['ID', 'question', 'answer']]\n",
    "train_df = pd.concat([train_df, phase1_truth], ignore_index=True)\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1860    Analyze the 5G wireless network drive-test user plane data and engineering parameters.\\nIdentify the reason for the throughput dropping below 600Mbps in certain road sections.\\nFrom the following 8 potential root causes, select the most likely one and enclose its number in \\boxed{{}} in the final answer.\\n\\nC1: The serving cell's downtilt angle is too large, causing weak coverage at the far end.\\nC2: The serving cell's coverage distance exceeds 1km, resulting in over-shooting.\\nC3: A neighboring cell provides higher throughput.\\nC4: Non-colocated co-frequency neighboring cells cause severe overlapping coverage.\\nC5: Frequent handovers degrade performance.\\nC6: Neighbor cell and serving cell have the same PCI mod 30, leading to interference.\\nC7: Test vehicle speed exceeds 40km/h, impacting user throughput.\\nC8: Average scheduled RBs are below 160, affecting throughput.\\n\\nGiven:\\n- The default electronic downtilt value is 255, representing a downtilt angle of 6 degrees. Other values represent the actual downtilt angle in degrees.\\n\\nBeam Scenario and Vertical Beamwidth Relationships:\\n- When the cell's Beam Scenario is set to Default or SCENARIO_1 to SCENARIO_5, the vertical beamwidth is 6 degrees.\\n- When the cell's Beam Scenario is set to SCENARIO_6 to SCENARIO_11, the vertical beamwidth is 12 degrees.\\n- When the cell's Beam Scenario is set to SCENARIO_12 or above, the vertical beamwidth is 25 degrees.\\n\\nUser plane drive test data as followsï¼š\\n\\nTimestamp|Longitude|Latitude|GPS Speed (km/h)|5G KPI PCell RF Serving PCI|5G KPI PCell RF Serving SS-RSRP [dBm]|5G KPI PCell RF Serving SS-SINR [dB]|5G KPI PCell Layer2 MAC DL Throughput [Mbps]|Measurement PCell Neighbor Cell Top Set(Cell Level) Top 1 PCI|Measurement PCell Neighbor Cell Top Set(Cell Level) Top 2 PCI|Measurement PCell Neighbor Cell Top Set(Cell Level) Top 3 PCI|Measurement PCell Neighbor Cell Top Set(Cell Level) Top 4 PCI|Measurement PCell Neighbor Cell Top Set(Cell Level) Top 5 PCI|Measurement PCell Neighbor Cell Top Set(Cell Level) Top 1 Filtered Tx BRSRP [dBm]|Measurement PCell Neighbor Cell Top Set(Cell Level) Top 2 Filtered Tx BRSRP [dBm]|Measurement PCell Neighbor Cell Top Set(Cell Level) Top 3 Filtered Tx BRSRP [dBm]|Measurement PCell Neighbor Cell Top Set(Cell Level) Top 4 Filtered Tx BRSRP [dBm]|Measurement PCell Neighbor Cell Top Set(Cell Level) Top 5 Filtered Tx BRSRP [dBm]|5G KPI PCell Layer1 DL RB Num (Including 0)\\n2025-05-07 14:24:22.000000|128.186911|32.584055|24|330|-86.56|22.51|964.18|591|660|-|-|-|-108.59|-114.77|-|-|-|202.7\\n2025-05-07 14:24:23.000000|128.186911|32.584055|5|330|-86.01|21.17|797.53|591|660|-|-|-|-107.89|-116.76|-|-|-|207.17\\n2025-05-07 14:24:24.000000|128.185869|32.585626|8|330|-84.77|20.68|803.45|591|660|-|-|-|-106.98|-112.41|-|-|-|197.95\\n2025-05-07 14:24:25.000000|128.186911|32.584055|10|660|-85.11|19.91|258.08|591|330|-|-|-|-105.19|-115.77|-|-|-|186.76\\n2025-05-07 14:24:26.000000|128.186911|32.584055|26|660|-87.1|22.06|330.57|591|330|-|-|-|-105.13|-115.44|-|-|-|210.03\\n2025-05-07 14:24:27.000000|128.186911|32.584055|4|660|-89.17|20.81|369.32|330|591|-|-|-|-107.06|-116.01|-|-|-|206.82\\n2025-05-07 14:24:28.000000|128.186911|32.584055|24|660|-89.4|20.93|396.12|330|591|-|-|-|-105.7|-115.88|-|-|-|209.6\\n2025-05-07 14:24:29.000000|128.186911|32.584055|36|591|-87.05|21.71|911.82|330|660|597|-|-|-104.9|-114.01|-118.61|-|-|205.73\\n2025-05-07 14:24:30.000000|128.186911|32.584055|16|591|-84.2|22.14|893.16|330|660|-|-|-|-104.21|-116.47|-|-|-|205.63\\n2025-05-07 14:24:31.000000|128.186911|32.584055|36|591|-86.77|20.48|993.16|330|660|597|-|-|-105.03|-115.41|-122.84|-|-|209.06\\n\\n\\nEngeneering parameters data as followsï¼š\\n\\ngNodeB ID|Cell ID|Longitude|Latitude|Mechanical Azimuth|Mechanical Downtilt|Digital Tilt|Digital Azimuth|Beam Scenario|Height|PCI|TxRx Mode|Max Transmit Power|Antenna Model\\n0000293|0|128.203985|32.589187|192|9|7|0|SCENARIO_9|3.0|660|64T64R|34.9|NR AAU 2\\n0033164|5|128.186986|32.585957|170|5|9|0|DEFAULT|51.1|591|32T32R|34.9|NR AAU 3\\n0033286|14|128.179948|32.574116|60|7|13|0|SCENARIO_3|56.0|597|32T32R|34.9|NR AAU 3\\n0033164|4|128.18644|32.584389|120|14|13|10|SCENARIO_4|52.9|330|32T32R|34.9|NR AAU 3\\n\n",
      "Name: question, dtype: str\n"
     ]
    }
   ],
   "source": [
    "# Review the question feature sample\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    print(train_df['question'].sample(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "CAUSE_DESC = {\n",
    "    \"C1\": \"downtilt too large\",\n",
    "    \"C2\": \"coverage > 1km\",\n",
    "    \"C3\": \"neighbor cell better throughput\",\n",
    "    \"C4\": \"non-colocated co-frequency overlap\",\n",
    "    \"C5\": \"frequent handovers\",\n",
    "    \"C6\": \"PCI mod30 conflict\",\n",
    "    \"C7\": \"speed > 40 km/h\",\n",
    "    \"C8\": \"RBs < 160\"\n",
    "}\n",
    "\n",
    "def extract_key_info(text: str) -> str:\n",
    "    info_parts = []\n",
    "\n",
    "    # --- Symptom ---\n",
    "    info_parts.append(\"Symptom: Throughput dropped below 600 Mbps in some road sections.\")\n",
    "\n",
    "    # --- Parse User Plane Data ---\n",
    "    user_plane_match = re.search(r'User plane drive test data as followsï¼š?\\s*\\n(.*?)(?:\\n\\n|\\Z)', text, re.DOTALL)\n",
    "    if user_plane_match:\n",
    "        table_str = user_plane_match.group(1).strip()\n",
    "        try:\n",
    "            table_str = table_str.replace('|', ',')\n",
    "            df_user = pd.read_csv(StringIO(table_str))\n",
    "\n",
    "            # Speed Analysis\n",
    "            speeds = df_user.get('GPS Speed (km/h)', pd.Series())\n",
    "            if not speeds.empty:\n",
    "                max_speed = float(speeds.max())\n",
    "                avg_speed = float(speeds.mean())\n",
    "                info_parts.append(f\"Speed: Max={max_speed:.1f} km/h, Avg={avg_speed:.1f} km/h\")\n",
    "                if max_speed > 40:\n",
    "                    info_parts.append(\"Speed exceeds 40 km/h â†’ potential C7\")\n",
    "\n",
    "            # Throughput Analysis\n",
    "            throughputs = df_user.get('5G KPI PCell Layer2 MAC DL Throughput [Mbps]', pd.Series())\n",
    "            if not throughputs.empty:\n",
    "                min_tp = float(throughputs.min())\n",
    "                avg_tp = float(throughputs.mean())\n",
    "                info_parts.append(f\"Throughput: Min={min_tp:.1f} Mbps, Avg={avg_tp:.1f} Mbps\")\n",
    "                if min_tp < 600:\n",
    "                    info_parts.append(\"Throughput consistently low\")\n",
    "\n",
    "            # RB Analysis\n",
    "            rbs = df_user.get('5G KPI PCell Layer1 DL RB Num (Including 0)', pd.Series())\n",
    "            if not rbs.empty:\n",
    "                avg_rbs = float(rbs.mean())\n",
    "                info_parts.append(f\"RBs: Avg={avg_rbs:.1f}\")\n",
    "                if avg_rbs < 160:\n",
    "                    info_parts.append(\"Avg RBs < 160 â†’ potential C8\")\n",
    "\n",
    "            # Handover Analysis\n",
    "            pcis = df_user.get('5G KPI PCell RF Serving PCI', pd.Series())\n",
    "            if not pcis.empty:\n",
    "                handover_count = (pcis != pcis.shift(1)).sum() - 1\n",
    "                unique_pcis = pcis.unique()\n",
    "                info_parts.append(f\"Handovers: {max(0, int(handover_count))} events\")\n",
    "                if handover_count > 3:\n",
    "                    info_parts.append(\"Frequent handovers â†’ potential C5\")\n",
    "                info_parts.append(f\"Serving PCIs: {list(unique_pcis.astype(int))}\")\n",
    "\n",
    "                # PCI Mod30 Conflict Check\n",
    "                neighbor_cols = [col for col in df_user.columns if 'Top' in col and 'PCI' in col]\n",
    "                if neighbor_cols:\n",
    "                    neighbor_pcis = pd.concat([df_user[col].dropna() for col in neighbor_cols]).unique()\n",
    "                    for pci_s in unique_pcis:\n",
    "                        for pci_n in neighbor_pcis:\n",
    "                            if int(pci_s) % 30 == int(pci_n) % 30 and pci_s != pci_n:\n",
    "                                info_parts.append(f\"PCI Mod30 Conflict: Serving {int(pci_s)} â†” Neighbor {int(pci_n)} â†’ potential C6\")\n",
    "                                break\n",
    "\n",
    "            # Neighbor Throughput Comparison (C3)\n",
    "            neighbor_tp_cols = [col for col in df_user.columns if 'Top' in col and 'Throughput' in col]\n",
    "            if neighbor_tp_cols:\n",
    "                neighbor_tps = df_user[neighbor_tp_cols].max(axis=1)\n",
    "                if not neighbor_tps.empty:\n",
    "                    avg_neighbor_tp = neighbor_tps.mean()\n",
    "                    if avg_neighbor_tp > avg_tp * 1.2:  # 20% higher\n",
    "                        info_parts.append(f\"Neighbor throughput avg={avg_neighbor_tp:.1f} Mbps > serving avg={avg_tp:.1f} â†’ potential C3\")\n",
    "\n",
    "        except Exception as e:\n",
    "            info_parts.append(\"Failed to parse user plane table; using fallback regex.\")\n",
    "\n",
    "    # --- Parse Engineering Parameters ---\n",
    "    eng_match = re.search(r'Engeneering parameters data as followsï¼š?\\s*\\n(.*?)(?:\\n\\n|\\Z)', text, re.DOTALL)\n",
    "    if eng_match:\n",
    "        table_str = eng_match.group(1).strip()\n",
    "        try:\n",
    "            table_str = table_str.replace('|', ',')\n",
    "            df_eng = pd.read_csv(StringIO(table_str))\n",
    "\n",
    "            # Effective Downtilt (C1)\n",
    "            digital_tilt = df_eng.get('Digital Tilt', pd.Series())\n",
    "            mech_tilt = df_eng.get('Mechanical Downtilt', pd.Series())\n",
    "            if not digital_tilt.empty and not mech_tilt.empty:\n",
    "                eff_tilts = []\n",
    "                for d, m in zip(digital_tilt, mech_tilt):\n",
    "                    d = float(d); m = float(m)\n",
    "                    eff_d = 6.0 if d == 255 else d\n",
    "                    eff_tilts.append(eff_d + m)\n",
    "                avg_downtilt = sum(eff_tilts) / len(eff_tilts)\n",
    "                info_parts.append(f\"Downtilt: Avg={avg_downtilt:.1f}Â°\")\n",
    "                if avg_downtilt > 10:  # Threshold based on typical deployment\n",
    "                    info_parts.append(\"Downtilt >10Â° â†’ potential C1 (weak far-end coverage)\")\n",
    "\n",
    "            # Beam Scenario â†’ Vertical Beamwidth\n",
    "            beam_scenarios = df_eng.get('Beam Scenario', pd.Series())\n",
    "            if not beam_scenarios.empty:\n",
    "                beam_widths = []\n",
    "                for bs in beam_scenarios:\n",
    "                    if pd.isna(bs): continue\n",
    "                    if 'SCENARIO_' in str(bs):\n",
    "                        num = int(re.search(r'SCENARIO_(\\d+)', str(bs)).group(1))\n",
    "                        if num <= 5:\n",
    "                            bw = 6\n",
    "                        elif 6 <= num <= 11:\n",
    "                            bw = 12\n",
    "                        else:\n",
    "                            bw = 25\n",
    "                    else:\n",
    "                        bw = 6  # DEFAULT\n",
    "                    beam_widths.append(bw)\n",
    "                info_parts.append(f\"Beamwidths: {beam_widths}\")\n",
    "                if any(bw > 12 for bw in beam_widths):\n",
    "                    info_parts.append(\"Wide beamwidths â†’ may indicate over-shooting or poor focus â†’ potential C2\")\n",
    "\n",
    "            # Colocation & Frequency (C4)\n",
    "            gnodeb_ids = df_eng.get('gNodeB ID', pd.Series()).dropna().unique()\n",
    "            if len(gnodeb_ids) == 1:\n",
    "                info_parts.append(\"Cells are colocated\")\n",
    "            else:\n",
    "                info_parts.append(\"Non-colocated cells â†’ potential C4 if co-frequency\")\n",
    "                info_parts.append(\"Non-colocated + co-frequency â†’ high risk of overlapping coverage â†’ potential C4\")\n",
    "\n",
    "            # Coverage Distance (C2)\n",
    "            distances = df_eng.get('Distance to Cell (m)', pd.Series())\n",
    "            if not distances.empty:\n",
    "                max_dist = float(distances.max())\n",
    "                info_parts.append(f\"Max Distance: {max_dist:.0f} m\")\n",
    "                if max_dist > 1000:\n",
    "                    info_parts.append(\"Coverage >1km â†’ potential C2 (over-shooting)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            info_parts.append(\"Failed to parse engineering table; using fallback regex.\")\n",
    "\n",
    "    # --- Fallback Regex for Critical Fields ---\n",
    "    if not any(\"Speed:\" in p for p in info_parts):\n",
    "        speed_match = re.search(r'speed\\s*[=:]\\s*(\\d+\\.?\\d*)\\s*km/h', text, re.IGNORECASE)\n",
    "        if speed_match:\n",
    "            info_parts.append(f\"Speed: Max={float(speed_match.group(1)):.1f} km/h\")\n",
    "            if float(speed_match.group(1)) > 40:\n",
    "                info_parts.append(\"Speed >40 km/h â†’ potential C7\")\n",
    "\n",
    "    if not any(\"RBs:\" in p for p in info_parts):\n",
    "        rb_match = re.search(r'(RBs?|resource blocks?)\\s*[=:]\\s*(\\d+)', text, re.IGNORECASE)\n",
    "        if rb_match:\n",
    "            avg_rbs = int(rb_match.group(2))\n",
    "            info_parts.append(f\"RBs: Avg={avg_rbs}\")\n",
    "            if avg_rbs < 160:\n",
    "                info_parts.append(\"RBs <160 â†’ potential C8\")\n",
    "\n",
    "    # --- Final Compact Summary ---\n",
    "    compact_q = \"Telco RCA Input:\\n\" + \"\\n\".join(info_parts)\n",
    "    return compact_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sft_example(question: str, answer: str) -> str:\n",
    "    cause_desc = CAUSE_DESC[answer]\n",
    "    compact_q = extract_key_info(question)\n",
    "\n",
    "    # Construct instruction-aware prompt using Qwen2.5-Instruct's chat format\n",
    "    return (\n",
    "        f\"<|im_start|>user\\n{compact_q}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\"\n",
    "        f\"Based on the diagnostic evidence, the most likely root cause is {cause_desc}. \"\n",
    "        f\"Final answer: \\\\boxed{{{answer}}}.<|im_end|>\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample enhanced prompt:\n",
      "<|im_start|>user\n",
      "Telco RCA Input:\n",
      "Symptom: Throughput dropped below 600 Mbps in some road sections.\n",
      "Speed: Max=39.0 km/h, Avg=23.4 km/h\n",
      "Throughput: Min=244.9 Mbps, Avg=604.2 Mbps\n",
      "Throughput consistently low\n",
      "RBs: Avg=190.2\n",
      "Handovers: 1 events\n",
      "Serving PCIs: [np.int64(430), np.int64(374)]\n",
      "Failed to parse user plane table; using fallback regex.\n",
      "Downtilt: Avg=12.9Â°\n",
      "Downtilt >10Â° â†’ potential C1 (weak far-end coverage)\n",
      "Beamwidths: [6, 6, 6, 6, 6, 12, 12]\n",
      "Non-colocated cells â†’ potential C4 if co-frequency\n",
      "Non-colocated + co-frequency â†’ high risk of overlapping coverage â†’ potential C4<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Based on the diagnostic evidence, the most likely root cause is downtilt too large. Final answer: \\boxed{C1}.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Build dataset using enhanced preprocessing\n",
    "sft_texts = []\n",
    "for _, row in train_df.iterrows():\n",
    "    q = str(row[\"question\"]).strip()\n",
    "    a = str(row[\"answer\"]).strip()\n",
    "    \n",
    "    # Skip invalid or out-of-scope answers\n",
    "    if a not in CAUSE_DESC:\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        txt = build_sft_example(q, a)\n",
    "        sft_texts.append(txt)\n",
    "    except Exception as e:\n",
    "        # Optional: log skipped samples for debugging\n",
    "        continue  # silently skip malformed entries\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": sft_texts})\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)  # reproducible split\n",
    "\n",
    "print(\"Sample enhanced prompt:\")\n",
    "print(dataset[\"train\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repr(dataset[\"train\"][1000][\"text\"][:1024]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 73,859,072 || all params: 1,617,573,376 || trainable%: 4.5660\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load model and tokenizer\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "# MODEL_NAME = '/kaggle/input/qwen2.5/transformers/1.5b-instruct/1'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Freeze base model\n",
    "model.requires_grad_(False)\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trl import SFTTrainer\n",
    "# from trl import DataCollatorForCompletionOnlyLM\n",
    "# from transformers import TrainingArguments\n",
    "\n",
    "# # 1. Define the Response Template for Qwen2.5 (ChatML)\n",
    "# # This identifies the start of the assistant's reply.\n",
    "# response_template = \"<|im_start|>assistant\\n\"\n",
    "\n",
    "# # 2. Setup the specialized collator\n",
    "# # mlm=False is implied here as it's specifically for causal completion.\n",
    "# data_collator = DataCollatorForCompletionOnlyLM(\n",
    "#     response_template=response_template, \n",
    "#     tokenizer=tokenizer\n",
    "# )\n",
    "\n",
    "# # 3. Training arguments tuned for T4Ã—2 (24GB VRAM)\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./qwen2.5-1.5b-rca\",\n",
    "#     per_device_train_batch_size=4,\n",
    "#     per_device_eval_batch_size=4,\n",
    "#     gradient_accumulation_steps=8,\n",
    "#     learning_rate=3e-4,\n",
    "#     num_train_epochs=1,\n",
    "#     logging_steps=1,\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=20,\n",
    "#     eval_strategy=\"steps\",\n",
    "#     eval_steps=20,\n",
    "#     bf16=True,  # T4 supports bf16, but use fp16=True if you see stability issues\n",
    "#     fp16=False,\n",
    "#     optim=\"adamw_torch\",\n",
    "#     lr_scheduler_type=\"cosine\",\n",
    "#     warmup_ratio=0.1,\n",
    "#     gradient_checkpointing=True,\n",
    "#     dataloader_num_workers=2,\n",
    "#     report_to=\"none\",\n",
    "#     remove_unused_columns=False, # Set to False when using SFTTrainer with raw text\n",
    "# )\n",
    "\n",
    "# # 4. Initialize SFTTrainer\n",
    "# # We pass the RAW dataset (dataset[\"train\"]) instead of tokenized_dataset.\n",
    "# # SFTTrainer will handle tokenization internally using the 'text' field.\n",
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=dataset[\"train\"],\n",
    "#     eval_dataset=dataset[\"test\"],\n",
    "#     dataset_text_field=\"text\",      # Name of the column containing the ChatML strings\n",
    "#     max_seq_length=1024,\n",
    "#     data_collator=data_collator,    # Our new completion-only collator\n",
    "# )\n",
    "\n",
    "# print(\"Starting training with Completion-Only masking...\")\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc82117e894d4264a4d18e3a596e9734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/2611 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b09e175ea4426b82178feaec6a413d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/653 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/training_args.py:2111: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a6c1464a5148d1aa59ef4a84060a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2611 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86eb96f0301c4db494e6d59fce38009c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/653 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Pre-tokenize dataset and train with evaluation every 10 steps\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from peft import PeftModel  # âœ… Import PeftModel to check type\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024,\n",
    "        return_special_tokens_mask=False\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    "    num_proc=1\n",
    ")\n",
    "\n",
    "# Data collator for causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen2.5-1.5b-rca\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=5e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=2,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=True,\n",
    "    load_best_model_at_end=False,\n",
    ")\n",
    "\n",
    "# SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save adapter-only version (optional, for later PEFT reuse)\n",
    "print(\"Saving LoRA adapters...\")\n",
    "trainer.save_model(\"./qwen2.5-1.5b-rca-adapters\")  # Saves only adapters\n",
    "tokenizer.save_pretrained(\"./qwen2.5-1.5b-rca-adapters\")\n",
    "\n",
    "# Save FULL MERGED MODEL (base + LoRA fused)\n",
    "print(\"Merging LoRA adapters into base model...\")\n",
    "if isinstance(model, PeftModel):\n",
    "    merged_model = model.merge_and_unload()  # Fuses LoRA weights into base model\n",
    "else:\n",
    "    merged_model = model  # Fallback if not PEFT-wrapped (unlikely)\n",
    "\n",
    "merged_output_dir = \"./qwen2.5-1.5b-rca-merged\"\n",
    "merged_model.save_pretrained(merged_output_dir)\n",
    "tokenizer.save_pretrained(merged_output_dir)\n",
    "\n",
    "print(f\" Full merged model saved to: {merged_output_dir}\")\n",
    "print(\"You can now load it with AutoModelForCausalLM.from_pretrained() without PEFT!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8cd67b8b62d4768a9ff9efde99788e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/2611 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1e8f8e512447f7ba16092f4e8151e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/653 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/training_args.py:2111: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6679af092d642feb9b9ddba050abe0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2611 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2734aeb9da764693a347df1537d1ae22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/653 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='82' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [82/82 3:54:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.269700</td>\n",
       "      <td>0.260451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.234400</td>\n",
       "      <td>0.237121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.226300</td>\n",
       "      <td>0.224982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.225300</td>\n",
       "      <td>0.221473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=82, training_loss=0.543404329113844, metrics={'train_runtime': 14227.6988, 'train_samples_per_second': 0.184, 'train_steps_per_second': 0.006, 'total_flos': 2.220530911936512e+16, 'train_loss': 0.543404329113844})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Cell 5: Pre-tokenize dataset and train with evaluation every 10 steps\n",
    "# from transformers import DataCollatorForLanguageModeling\n",
    "# from trl import SFTTrainer\n",
    "# from transformers import TrainingArguments\n",
    "\n",
    "# # Tokenize function\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(\n",
    "#         examples[\"text\"],\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",\n",
    "#         max_length=1024,  # compressed prompts are short; 1024 is safe & efficient\n",
    "#         return_special_tokens_mask=False  # not needed for causal LM\n",
    "#     )\n",
    "\n",
    "# print(\"Tokenizing dataset...\")\n",
    "# tokenized_dataset = dataset.map(\n",
    "#     tokenize_function,\n",
    "#     batched=True,\n",
    "#     remove_columns=[\"text\"],  # Free memory\n",
    "#     num_proc=1  # Kaggle CPU cores are limited\n",
    "# )\n",
    "\n",
    "# # Data collator for causal language modeling\n",
    "# data_collator = DataCollatorForLanguageModeling(\n",
    "#     tokenizer=tokenizer,\n",
    "#     mlm=False  # Important: this is a causal LM, not masked LM\n",
    "# )\n",
    "\n",
    "# # Training arguments tuned for T4Ã—2 (24GB VRAM)\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./qwen2.5-1.5b-rca\",\n",
    "#     per_device_train_batch_size=4,           # Fit in T4 VRAM\n",
    "#     per_device_eval_batch_size=4,\n",
    "#     gradient_accumulation_steps=8,           # Effective batch = 32\n",
    "#     learning_rate=1e-4,\n",
    "#     num_train_epochs=1,\n",
    "#     logging_steps=1,\n",
    "#     save_strategy=\"steps\",                   # Optional: save more frequently\n",
    "#     save_steps=20,                           # Save checkpoint every 10 steps\n",
    "#     eval_strategy=\"steps\",                   \n",
    "#     eval_steps=20,                           \n",
    "#     bf16=True,                              \n",
    "#     fp16=False,\n",
    "#     optim=\"adamw_torch\",\n",
    "#     lr_scheduler_type=\"cosine\",\n",
    "#     warmup_ratio=0.1,\n",
    "#     gradient_checkpointing=True,            \n",
    "#     dataloader_num_workers=2,\n",
    "#     report_to=\"none\",\n",
    "#     remove_unused_columns=True,\n",
    "#     load_best_model_at_end=False,           # Disable if using step-based eval without metric\n",
    "#     greater_is_better=False,                # Not used unless you define a metric\n",
    "# )\n",
    "\n",
    "# # SFTTrainer WITHOUT tokenizer or formatting_func\n",
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_dataset[\"train\"],\n",
    "#     eval_dataset=tokenized_dataset[\"test\"],\n",
    "#     data_collator=data_collator,\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Fine-tuning (SFT) is the first critical step in transforming a raw, \"base\" language model into an \"instruct\" model that can actually follow user commands. While a base model is trained to simply predict the next word on the internet, an SFT-trained model learns to act as a helpful assistant. Implemented using the **SFTTrainer** from HuggingFace's `trl` (**_Transformer Reinforcement Learning_**) library. SFT reduces the likelihood of the model \"hallucinating\" the continuation of the prompt by keeping it focused on the answer. \n",
    "\n",
    "The `lr_scheduler_type` parameter determines how the learning rate changes over the course of training. Set argument for this parameter is crucial in ensuring the model \"settles\" into its new task without erasing the foundational knowledge it gained during pre-training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: FAST Batched Inference with Progress Bar + Immediate Parsing\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# âœ… Load the FULL MERGED model (no PEFT needed!)\n",
    "print(\"Loading fine-tuned merged model for inference...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./qwen2.5-1.5b-rca-merged\",\n",
    "    device_map=\"auto\",        # Automatically uses GPU if available\n",
    "    torch_dtype=torch.bfloat16  # Match training dtype (bf16)\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./qwen2.5-1.5b-rca-merged\")\n",
    "\n",
    "# Ensure extract_key_info and CAUSE_DESC are defined (from earlier cells)\n",
    "# If not, re-import or re-define them here\n",
    "\n",
    "def map_output_to_root_cause(text: str) -> str:\n",
    "    \"\"\"Lightweight version for inference-time parsing\"\"\"\n",
    "    # Priority 1: Extract \\boxed{C#}\n",
    "    boxed_match = re.search(r'\\\\boxed\\{(C[1-8])\\}', text)\n",
    "    if boxed_match:\n",
    "        return boxed_match.group(1)\n",
    "    \n",
    "    # Priority 2: Direct C# mention\n",
    "    direct_match = re.search(r'\\b(C[1-8])\\b', text)\n",
    "    if direct_match:\n",
    "        return direct_match.group(1)\n",
    "    \n",
    "    # Priority 3: Heuristic fallback using cause descriptions\n",
    "    text_lower = text.lower()\n",
    "    if \"downtilt too large\" in text_lower or \"tilt too large\" in text_lower: return \"C1\"\n",
    "    if \"coverage > 1km\" in text_lower or \">1km\" in text_lower: return \"C2\"\n",
    "    if \"neighbor cell better throughput\" in text_lower: return \"C3\"\n",
    "    if \"non-colocated\" in text_lower and (\"co-frequency\" in text_lower or \"overlap\" in text_lower): return \"C4\"\n",
    "    if \"frequent handover\" in text_lower: return \"C5\"\n",
    "    if \"pci mod30\" in text_lower or \"mod30 conflict\" in text_lower: return \"C6\"\n",
    "    if \"speed > 40\" in text_lower or (\"speed\" in text_lower and re.search(r'[4-9]\\d\\s*km/h', text_lower)): return \"C7\"\n",
    "    if \"rb\" in text_lower and (\"<160\" in text_lower or \"below 160\" in text_lower or \"avg rbs\" in text_lower): return \"C8\"\n",
    "    \n",
    "    return \"C1\"  # safe default\n",
    "\n",
    "\n",
    "def generate_responses_batched(df: pd.DataFrame, batch_size=64, num_return_sequences=4):\n",
    "    all_rows = []\n",
    "    device = model.device\n",
    "\n",
    "    # Precompute prompts using the SAME extract_key_info used in training\n",
    "    all_prompts = []\n",
    "    all_original_ids = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # CRITICAL: Use the enhanced extract_key_info\n",
    "        compact_q = extract_key_info(str(row[\"question\"]).strip())\n",
    "        \n",
    "        # Format as chat for Qwen2.5-Instruct\n",
    "        messages = [{\"role\": \"user\", \"content\": compact_q}]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Repeat for ensemble-style sampling\n",
    "        all_prompts.extend([prompt] * num_return_sequences)\n",
    "        all_original_ids.extend([row['ID']] * num_return_sequences)\n",
    "\n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(all_prompts), batch_size), desc=f\"Inference ({df.iloc[0]['ID'].split('_')[0]})\"):\n",
    "        batch_prompts = all_prompts[i:i + batch_size]\n",
    "        batch_orig_ids = all_original_ids[i:i + batch_size]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            batch_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1024\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # Remove input prompt from output\n",
    "        input_lengths = inputs.input_ids.shape[1]\n",
    "        decoded = tokenizer.batch_decode(\n",
    "            outputs[:, input_lengths:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # Parse and store\n",
    "        for j, text in enumerate(decoded):\n",
    "            cause = map_output_to_root_cause(text.strip())\n",
    "            all_rows.append({\n",
    "                \"original_ID\": batch_orig_ids[j],\n",
    "                \"sample_ID\": f\"{batch_orig_ids[j]}_{j % num_return_sequences + 1}\",\n",
    "                \"raw_output\": text.strip(),\n",
    "                \"predicted_cause\": cause\n",
    "            })\n",
    "\n",
    "    return all_rows\n",
    "\n",
    "\n",
    "# Run inference\n",
    "print(\"Running batched inference on Phase 1...\")\n",
    "phase1_rows = generate_responses_batched(phase1_test, batch_size=64, num_return_sequences=4)\n",
    "\n",
    "print(\"Running batched inference on Phase 2...\")\n",
    "phase2_rows = generate_responses_batched(phase2_test, batch_size=64, num_return_sequences=4)\n",
    "\n",
    "all_pred_rows = phase1_rows + phase2_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 6: FAST Batched Inference with Progress Bar + Immediate Parsing\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# import re\n",
    "\n",
    "# # Ensure extract_key_info and CAUSE_DESC are defined (from earlier cells)\n",
    "# # If not, re-import or re-define them here\n",
    "\n",
    "# def map_output_to_root_cause(text: str) -> str:\n",
    "#     \"\"\"Lightweight version for inference-time parsing\"\"\"\n",
    "#     # Priority 1: Extract \\boxed{C#}\n",
    "#     boxed_match = re.search(r'\\\\boxed\\{(C[1-8])\\}', text)\n",
    "#     if boxed_match:\n",
    "#         return boxed_match.group(1)\n",
    "    \n",
    "#     # Priority 2: Direct C# mention\n",
    "#     direct_match = re.search(r'\\b(C[1-8])\\b', text)\n",
    "#     if direct_match:\n",
    "#         return direct_match.group(1)\n",
    "    \n",
    "#     # Priority 3: Heuristic fallback using cause descriptions\n",
    "#     text_lower = text.lower()\n",
    "#     if \"downtilt too large\" in text_lower or \"tilt too large\" in text_lower: return \"C1\"\n",
    "#     if \"coverage > 1km\" in text_lower or \">1km\" in text_lower: return \"C2\"\n",
    "#     if \"neighbor cell better throughput\" in text_lower: return \"C3\"\n",
    "#     if \"non-colocated\" in text_lower and (\"co-frequency\" in text_lower or \"overlap\" in text_lower): return \"C4\"\n",
    "#     if \"frequent handover\" in text_lower: return \"C5\"\n",
    "#     if \"pci mod30\" in text_lower or \"mod30 conflict\" in text_lower: return \"C6\"\n",
    "#     if \"speed > 40\" in text_lower or (\"speed\" in text_lower and re.search(r'[4-9]\\d\\s*km/h', text_lower)): return \"C7\"\n",
    "#     if \"rb\" in text_lower and (\"<160\" in text_lower or \"below 160\" in text_lower or \"avg rbs\" in text_lower): return \"C8\"\n",
    "    \n",
    "#     return \"C1\"  # safe default\n",
    "\n",
    "\n",
    "# def generate_responses_batched(df: pd.DataFrame, batch_size=64, num_return_sequences=4):\n",
    "#     all_rows = []\n",
    "#     device = model.device\n",
    "\n",
    "#     # Precompute prompts using the SAME extract_key_info used in training\n",
    "#     all_prompts = []\n",
    "#     all_original_ids = []\n",
    "\n",
    "#     for _, row in df.iterrows():\n",
    "#         # CRITICAL: Use the enhanced extract_key_info\n",
    "#         compact_q = extract_key_info(str(row[\"question\"]).strip())\n",
    "        \n",
    "#         # Format as chat for Qwen2.5-Instruct\n",
    "#         messages = [{\"role\": \"user\", \"content\": compact_q}]\n",
    "#         prompt = tokenizer.apply_chat_template(\n",
    "#             messages,\n",
    "#             tokenize=False,\n",
    "#             add_generation_prompt=True\n",
    "#         )\n",
    "        \n",
    "#         # Repeat for ensemble-style sampling\n",
    "#         all_prompts.extend([prompt] * num_return_sequences)\n",
    "#         all_original_ids.extend([row['ID']] * num_return_sequences)\n",
    "\n",
    "#     # Process in batches\n",
    "#     for i in tqdm(range(0, len(all_prompts), batch_size), desc=f\"Inference ({df.iloc[0]['ID'].split('_')[0]})\"):\n",
    "#         batch_prompts = all_prompts[i:i + batch_size]\n",
    "#         batch_orig_ids = all_original_ids[i:i + batch_size]\n",
    "\n",
    "#         inputs = tokenizer(\n",
    "#             batch_prompts,\n",
    "#             return_tensors=\"pt\",\n",
    "#             padding=True,\n",
    "#             truncation=True,\n",
    "#             max_length=1024  # Increased to accommodate richer context\n",
    "#         ).to(device)\n",
    "\n",
    "#         with torch.inference_mode():\n",
    "#             outputs = model.generate(\n",
    "#                 **inputs,\n",
    "#                 max_new_tokens=256,          # Reduced (answers are short)\n",
    "#                 do_sample=True,\n",
    "#                 temperature=0.7,\n",
    "#                 top_p=0.9,\n",
    "#                 pad_token_id=tokenizer.eos_token_id,\n",
    "#                 eos_token_id=tokenizer.eos_token_id\n",
    "#             )\n",
    "\n",
    "#         # Remove input prompt from output\n",
    "#         input_lengths = inputs.input_ids.shape[1]\n",
    "#         decoded = tokenizer.batch_decode(\n",
    "#             outputs[:, input_lengths:],\n",
    "#             skip_special_tokens=True\n",
    "#         )\n",
    "\n",
    "#         # Parse and store\n",
    "#         for j, text in enumerate(decoded):\n",
    "#             cause = map_output_to_root_cause(text.strip())\n",
    "#             all_rows.append({\n",
    "#                 \"original_ID\": batch_orig_ids[j],\n",
    "#                 \"sample_ID\": f\"{batch_orig_ids[j]}_{j % num_return_sequences + 1}\",\n",
    "#                 \"raw_output\": text.strip(),\n",
    "#                 \"predicted_cause\": cause\n",
    "#             })\n",
    "\n",
    "#     return all_rows\n",
    "\n",
    "\n",
    "# # Run inference\n",
    "# print(\"Running batched inference on Phase 1...\")\n",
    "# phase1_rows = generate_responses_batched(phase1_test, batch_size=64, num_return_sequences=4)\n",
    "\n",
    "# print(\"Running batched inference on Phase 2...\")\n",
    "# phase2_rows = generate_responses_batched(phase2_test, batch_size=64, num_return_sequences=4)\n",
    "\n",
    "# all_pred_rows = phase1_rows + phase2_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Build final submission with voting over 4 samples per test case\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Reuse CAUSE_DESC if not already in scope\n",
    "CAUSE_DESC = {\n",
    "    \"C1\": \"downtilt too large\",\n",
    "    \"C2\": \"coverage > 1km\",\n",
    "    \"C3\": \"neighbor cell better throughput\",\n",
    "    \"C4\": \"non-colocated co-frequency overlap\",\n",
    "    \"C5\": \"frequent handovers\",\n",
    "    \"C6\": \"PCI mod30 conflict\",\n",
    "    \"C7\": \"speed > 40 km/h\",\n",
    "    \"C8\": \"RBs < 160\"\n",
    "}\n",
    "\n",
    "def map_output_to_root_cause(text: str) -> str:\n",
    "    \"\"\"Robust parser for model output (used during inference fallback)\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Priority 1: \\boxed{Cx}\n",
    "    boxed_match = re.search(r'\\\\boxed\\{(C[1-8])\\}', text)\n",
    "    if boxed_match:\n",
    "        return boxed_match.group(1)\n",
    "    \n",
    "    # Priority 2: standalone Cx\n",
    "    direct_match = re.search(r'\\b(C[1-8])\\b', text)\n",
    "    if direct_match:\n",
    "        return direct_match.group(1)\n",
    "    \n",
    "    # Priority 3: keyword triggers (aligned with cause descriptions)\n",
    "    triggers = {\n",
    "        \"C1\": [r\"downtilt.*large\", r\"tilt.*too.*large\", r\"effective.*downtilt.*[7-9]\\d*\"],\n",
    "        \"C2\": [r\"coverage.*>.*1.?km\", r\"distance.*>.*1000\", r\"over.?shoot\"],\n",
    "        \"C3\": [r\"neighbor.*cell.*better.*throughput\", r\"neighboring.*higher.*throughput\"],\n",
    "        \"C4\": [r\"non.?colocated.*co.?frequency\", r\"overlap.*coverage.*non.?colocated\"],\n",
    "        \"C5\": [r\"frequent.*handover\", r\"handovers.*\\b([5-9]\\d|\\d{3,})\\b\"],\n",
    "        \"C6\": [r\"pci.*mod.*30.*conflict\", r\"mod30.*conflict\", r\"pci.*mod30\"],\n",
    "        \"C7\": [r\"speed.*>.*40.*km/h\", r\"max.*speed.*>.*40\"],\n",
    "        \"C8\": [r\"rb.*<.*160\", r\"avg.*rb.*<.*160\", r\"resource.*blocks.*below.*160\"]\n",
    "    }\n",
    "    \n",
    "    for cause, patterns in triggers.items():\n",
    "        for pat in patterns:\n",
    "            if re.search(pat, text_lower):\n",
    "                return cause\n",
    "                \n",
    "    return \"C7\"  # safe default\n",
    "\n",
    "\n",
    "# Convert inference results to DataFrame\n",
    "pred_df = pd.DataFrame(all_pred_rows)\n",
    "\n",
    "# Ensure 'original_ID' exists (e.g., \"P1_001\")\n",
    "assert \"original_ID\" in pred_df.columns, \"Missing 'original_ID' in inference output\"\n",
    "assert \"raw_output\" in pred_df.columns, \"Missing 'raw_output' in inference output\"\n",
    "\n",
    "# Re-parse predictions (optional but safe: in case parsing was skipped during inference)\n",
    "pred_df[\"predicted_cause\"] = pred_df[\"raw_output\"].apply(map_output_to_root_cause)\n",
    "\n",
    "# Group by base test ID (e.g., \"P1_001\") and vote\n",
    "final_predictions = {}\n",
    "for base_id, group in pred_df.groupby(\"original_ID\"):\n",
    "    causes = group[\"predicted_cause\"].tolist()\n",
    "    counter = Counter(causes)\n",
    "    # Get most common; if tie, pick the first one encountered (most_common is stable)\n",
    "    final_cause = counter.most_common(1)[0][0]\n",
    "    final_predictions[base_id] = final_cause\n",
    "\n",
    "# Load sample submission\n",
    "sample_sub = pd.read_csv(\"SampleSubmission.csv\")\n",
    "submission = sample_sub.copy()\n",
    "\n",
    "# Map predictions to submission format: \\boxed{Cx}\n",
    "submission[\"Qwen2.5-1.5B-Instruct\"] = (\n",
    "    submission[\"ID\"]\n",
    "    .map(final_predictions)\n",
    "    .apply(lambda c: f\"\\\\boxed{{{c}}}\" if pd.notna(c) else \"\\\\boxed{C7}\")\n",
    ")\n",
    "\n",
    "# Handle any missing IDs (should not happen if test sets match)\n",
    "missing = submission[\"Qwen2.5-1.5B-Instruct\"].isna().sum()\n",
    "if missing > 0:\n",
    "    print(f\"  Warning: {missing} IDs had no prediction. Filling with \\\\boxed{{C7}}.\")\n",
    "    submission[\"Qwen2.5-1.5B-Instruct\"].fillna(\"\\\\boxed{C7}\", inplace=True)\n",
    "\n",
    "# Save\n",
    "output_file = \"lightning_submission_track3_forcompletiononly.csv\"\n",
    "submission.to_csv(output_file, index=False)\n",
    "print(f\"âœ… Submission saved to '{output_file}' with {len(submission)} rows.\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9308498,
     "sourceId": 14572366,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 164048,
     "modelInstanceId": 141458,
     "sourceId": 166245,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
